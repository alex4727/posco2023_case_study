{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from resnet import resnet18\n",
    "from utils import AverageMeter, accuracy, add_to_confusion_matrix, get_per_class_results, make_deterministic, save_ckpt, load_ckpt\n",
    "\n",
    "use_pretrained = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_DIR = os.getcwd()\n",
    "train_dataset_path = os.path.join(ROOT_DIR, \"posco_data/places10_LT/train\")\n",
    "valid_dataset_path = os.path.join(ROOT_DIR,\"posco_data/places10_LT/valid\")\n",
    "\n",
    "batch_size = 64\n",
    "total_epochs = 30\n",
    "lr_steps = [10, 20, 25]\n",
    "turn_on_wandb = True\n",
    "run_name = \"resnet18_places10_LT_undersampling\"\n",
    "\n",
    "make_deterministic(random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from randaugment import rand_augment_transform, GaussianBlur_simclr\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, transform, use_randaug=False):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.use_randaug = use_randaug\n",
    "        if self.use_randaug:\n",
    "            rgb_mean = (0.485, 0.456, 0.406)\n",
    "            ra_params = dict(translate_const=int(224 * 0.45), img_mean=tuple([min(255, round(255 * x)) for x in rgb_mean]), )\n",
    "            normalize = self.transform.transforms[-1] # get normalize layer\n",
    "            self.aug1 = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224, scale=(0.08, 1.)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.ColorJitter(0.4, 0.4, 0.4, 0.0)\n",
    "                ], p=1.0),\n",
    "                rand_augment_transform('rand-n{}-m{}-mstd0.5'.format(2, 10), ra_params),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "            self.aug2 = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.ColorJitter(0.4, 0.4, 0.4, 0.0)  # not strengthened\n",
    "                ], p=1.0),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.RandomApply([GaussianBlur_simclr([.1, 2.])], p=0.5),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        print(\"Loading dataset...\")\n",
    "        self.load_dataset()\n",
    "\n",
    "    def do_under_sampling(self, data):\n",
    "        indices = []\n",
    "        min_count = min([data.targets.count(i) for i in range(len(data.classes))])\n",
    "        for i in range(len(data.classes)):\n",
    "            target_indices = [j for j, x in enumerate(data.targets) if x == i]\n",
    "            indices += target_indices[:min_count]\n",
    "        return indices\n",
    "\n",
    "    def load_dataset(self):\n",
    "        self.data = ImageFolder(self.dataset_path)\n",
    "        \n",
    "        # TODO UNDERSAMPLING CODE\n",
    "        self.data = torch.utils.data.Subset(self.data, self.do_under_sampling(self.data))\n",
    "        \n",
    "    def do_transform(self, img):\n",
    "        if self.use_randaug:\n",
    "            r = random.random()\n",
    "            if r < 0.5:\n",
    "                img = self.aug1(img)\n",
    "            else:\n",
    "                img = self.aug2(img)\n",
    "        else:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        img = self.do_transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "if use_pretrained:\n",
    "    model = resnet18(pretrained=True, num_classes=1000).to(device)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 10).to(device)\n",
    "else:\n",
    "    model = resnet18(num_classes=10).to(device)\n",
    "\n",
    "\n",
    "# Make Dataset & DataLoader\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = CustomDataset(dataset_path=train_dataset_path, transform=train_transforms, use_randaug=False)\n",
    "valid_dataset = CustomDataset(dataset_path=valid_dataset_path, transform=valid_transforms, use_randaug=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "print(\"Train dataset length:\", len(train_dataset))\n",
    "print(\"Valid dataset length:\", len(valid_dataset))\n",
    "\n",
    "# Make Optimizer & Loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), 1e-2, momentum=0.9, weight_decay=2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_steps, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, train_loader, model, criterion, optimizer, scheduler, epoch, turn_on_wandb=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses, top1, top5 = AverageMeter(device), AverageMeter(device), AverageMeter(device)\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        prec1, prec5 = accuracy(outputs, labels , topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(prec1.item(), images.size(0))\n",
    "        top5.update(prec5.item(), images.size(0))\n",
    "\n",
    "    end_time = time.time()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"==================== Train Summary: Epoch {epoch+1} ====================\", flush=True)\n",
    "    print(f\"Train Epoch Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(end_time - start_time))}\", flush=True)\n",
    "    print(f\"Loss: {losses.avg:.2f}\\t Prec@1: {top1.avg:.2f}\\t Prec@5: {top5.avg:.2f}\", flush=True)\n",
    "    if turn_on_wandb:\n",
    "        wandb.log({\"train/loss\": losses.avg, \"train/top1\": top1.avg, \"train/top5\": top5.avg}, step=epoch+1)\n",
    "\n",
    "\n",
    "def validate(device, valid_loader, model, criterion, epoch, turn_on_wandb=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses, top1, top5 = AverageMeter(device), AverageMeter(device), AverageMeter(device)\n",
    "    model.eval()\n",
    "    confusion_matrix = torch.zeros(10, 10).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            prec1, prec5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "            confusion_matrix = add_to_confusion_matrix(confusion_matrix, outputs, labels)\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(prec1.item(), images.size(0))\n",
    "            top5.update(prec5.item(), images.size(0))\n",
    "\n",
    "    end_time = time.time()\n",
    "    per_class_results = get_per_class_results(confusion_matrix)\n",
    "    print(f\"==================== Valid Summary: Epoch {epoch+1} ====================\", flush=True)\n",
    "    print(f\"Valid Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(end_time - start_time))}\", flush=True)\n",
    "    print(f\"Loss: {losses.avg:.2f}\\t Prec@1: {top1.avg:.2f}\\t Prec@5: {top5.avg:.2f}\", flush=True)\n",
    "    if turn_on_wandb:\n",
    "        wandb.log({\"valid/loss\": losses.avg, \"valid/top1\": top1.avg, \"valid/top5\": top5.avg}, step=epoch+1)\n",
    "    return top1.avg, per_class_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if turn_on_wandb:\n",
    "    wandb.init(project=\"posco2023\", name=run_name)\n",
    "\n",
    "# Main Loop\n",
    "best_top1, best_top1_epoch, best_per_class_results = 0, 0, None\n",
    "for epoch in range(total_epochs):\n",
    "    train(device, train_loader, model, criterion, optimizer, scheduler, epoch, turn_on_wandb=True)\n",
    "    top1, per_class_results = validate(device, valid_loader, model, criterion, epoch, turn_on_wandb=True)\n",
    "    if top1 > best_top1:\n",
    "        best_top1 = top1\n",
    "        best_top1_epoch = epoch+1\n",
    "        best_per_class_results = per_class_results\n",
    "        save_ckpt(epoch=epoch+1, model=model, per_class_results=per_class_results, run_name=run_name)\n",
    "        \n",
    "    print(f\"Best Prec@1: {best_top1:.2f} at epoch {best_top1_epoch}\", flush=True)\n",
    "\n",
    "# Print Best Results\n",
    "print(f\"Best per class results: {best_per_class_results}\", flush=True)\n",
    "\n",
    "if turn_on_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and testing\n",
    "model_for_eval = resnet18(num_classes=10).to(device)\n",
    "load_ckpt(model=model_for_eval, run_name=run_name)\n",
    "validate(device, valid_loader, model_for_eval, criterion, epoch=0, turn_on_wandb=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
